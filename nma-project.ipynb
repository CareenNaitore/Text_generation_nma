{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "074fb514",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-25T20:51:56.479356Z",
     "iopub.status.busy": "2022-07-25T20:51:56.478419Z",
     "iopub.status.idle": "2022-07-25T20:51:56.604222Z",
     "shell.execute_reply": "2022-07-25T20:51:56.603231Z"
    },
    "papermill": {
     "duration": 0.136979,
     "end_time": "2022-07-25T20:51:56.607920",
     "exception": false,
     "start_time": "2022-07-25T20:51:56.470941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/huggingface-bert/bert-base-multilingual-cased/config.json\n",
      "/kaggle/input/huggingface-bert/bert-base-multilingual-cased/tokenizer.json\n",
      "/kaggle/input/huggingface-bert/bert-base-multilingual-cased/tf_model.h5\n",
      "/kaggle/input/huggingface-bert/bert-base-multilingual-cased/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/bert-base-multilingual-cased/modelcard.json\n",
      "/kaggle/input/huggingface-bert/bert-base-multilingual-cased/vocab.txt\n",
      "/kaggle/input/huggingface-bert/bert-large-uncased/config.json\n",
      "/kaggle/input/huggingface-bert/bert-large-uncased/tokenizer.json\n",
      "/kaggle/input/huggingface-bert/bert-large-uncased/tf_model.h5\n",
      "/kaggle/input/huggingface-bert/bert-large-uncased/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/bert-large-uncased/modelcard.json\n",
      "/kaggle/input/huggingface-bert/bert-large-uncased/vocab.txt\n",
      "/kaggle/input/huggingface-bert/bert-large-uncased/whole-word-masking/._bert_config.json\n",
      "/kaggle/input/huggingface-bert/bert-large-uncased/whole-word-masking/bert_config.json\n",
      "/kaggle/input/huggingface-bert/bert-large-uncased/whole-word-masking/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/bert-large-cased/config.json\n",
      "/kaggle/input/huggingface-bert/bert-large-cased/tokenizer.json\n",
      "/kaggle/input/huggingface-bert/bert-large-cased/tf_model.h5\n",
      "/kaggle/input/huggingface-bert/bert-large-cased/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/bert-large-cased/modelcard.json\n",
      "/kaggle/input/huggingface-bert/bert-large-cased/vocab.txt\n",
      "/kaggle/input/huggingface-bert/bert-large-cased/whole-word-masking/._bert_config.json\n",
      "/kaggle/input/huggingface-bert/bert-large-cased/whole-word-masking/bert_config.json\n",
      "/kaggle/input/huggingface-bert/bert-large-cased/whole-word-masking/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/cl-tohoku/bert-base-japanese-whole-word-masking/config.json\n",
      "/kaggle/input/huggingface-bert/cl-tohoku/bert-base-japanese-whole-word-masking/tf_model.h5\n",
      "/kaggle/input/huggingface-bert/cl-tohoku/bert-base-japanese-whole-word-masking/tokenizer_config.json\n",
      "/kaggle/input/huggingface-bert/cl-tohoku/bert-base-japanese-whole-word-masking/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/cl-tohoku/bert-base-japanese-whole-word-masking/vocab.txt\n",
      "/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/rust_model.ot\n",
      "/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/config.json\n",
      "/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/tf_model.h5\n",
      "/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/tokenizer_config.json\n",
      "/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/modelcard.json\n",
      "/kaggle/input/huggingface-bert/dbmdz/bert-large-cased-finetuned-conll03-english/vocab.txt\n",
      "/kaggle/input/huggingface-bert/bert-large-cased-whole-word-masking/config.json\n",
      "/kaggle/input/huggingface-bert/bert-large-cased-whole-word-masking/tokenizer.json\n",
      "/kaggle/input/huggingface-bert/bert-large-cased-whole-word-masking/tf_model.h5\n",
      "/kaggle/input/huggingface-bert/bert-large-cased-whole-word-masking/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/bert-large-cased-whole-word-masking/modelcard.json\n",
      "/kaggle/input/huggingface-bert/bert-large-cased-whole-word-masking/vocab.txt\n",
      "/kaggle/input/huggingface-bert/bert-base-cased/config.json\n",
      "/kaggle/input/huggingface-bert/bert-base-cased/tokenizer.json\n",
      "/kaggle/input/huggingface-bert/bert-base-cased/tf_model.h5\n",
      "/kaggle/input/huggingface-bert/bert-base-cased/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/bert-base-cased/modelcard.json\n",
      "/kaggle/input/huggingface-bert/bert-base-cased/vocab.txt\n",
      "/kaggle/input/huggingface-bert/bert-base-cased/flax_model.msgpack\n",
      "/kaggle/input/huggingface-bert/bert-base-german-cased/config.json\n",
      "/kaggle/input/huggingface-bert/bert-base-german-cased/tokenizer.json\n",
      "/kaggle/input/huggingface-bert/bert-base-german-cased/tf_model.h5\n",
      "/kaggle/input/huggingface-bert/bert-base-german-cased/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/bert-base-german-cased/modelcard.json\n",
      "/kaggle/input/huggingface-bert/bert-base-multilingual-uncased/config.json\n",
      "/kaggle/input/huggingface-bert/bert-base-multilingual-uncased/tokenizer.json\n",
      "/kaggle/input/huggingface-bert/bert-base-multilingual-uncased/tf_model.h5\n",
      "/kaggle/input/huggingface-bert/bert-base-multilingual-uncased/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/bert-base-multilingual-uncased/modelcard.json\n",
      "/kaggle/input/huggingface-bert/bert-base-multilingual-uncased/vocab.txt\n",
      "/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/config.json\n",
      "/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/tokenizer_config.json\n",
      "/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/modelcard.json\n",
      "/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/special_tokens_map.json\n",
      "/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/vocab.txt\n",
      "/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/added_tokens.json\n",
      "/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/saved_model/saved_model.pb\n",
      "/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/saved_model/variables/variables.index\n",
      "/kaggle/input/huggingface-bert/deepset/bert-large-uncased-whole-word-masking-squad2/saved_model/variables/variables.data-00000-of-00001\n",
      "/kaggle/input/huggingface-bert/bert-base-chinese/config.json\n",
      "/kaggle/input/huggingface-bert/bert-base-chinese/tokenizer.json\n",
      "/kaggle/input/huggingface-bert/bert-base-chinese/tf_model.h5\n",
      "/kaggle/input/huggingface-bert/bert-base-chinese/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/bert-base-chinese/modelcard.json\n",
      "/kaggle/input/huggingface-bert/bert-base-chinese/vocab.txt\n",
      "/kaggle/input/huggingface-bert/bert-base-uncased/rust_model.ot\n",
      "/kaggle/input/huggingface-bert/bert-base-uncased/config.json\n",
      "/kaggle/input/huggingface-bert/bert-base-uncased/tokenizer.json\n",
      "/kaggle/input/huggingface-bert/bert-base-uncased/tf_model.h5\n",
      "/kaggle/input/huggingface-bert/bert-base-uncased/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/bert-base-uncased/modelcard.json\n",
      "/kaggle/input/huggingface-bert/bert-base-uncased/vocab.txt\n",
      "/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-uncased/config.json\n",
      "/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-uncased/tokenizer_config.json\n",
      "/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-uncased/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-uncased/special_tokens_map.json\n",
      "/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-uncased/vocab.txt\n",
      "/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-uncased/added_tokens.json\n",
      "/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-cased/config.json\n",
      "/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-cased/tokenizer_config.json\n",
      "/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-cased/pytorch_model.bin\n",
      "/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-cased/special_tokens_map.json\n",
      "/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-cased/vocab.txt\n",
      "/kaggle/input/huggingface-bert/dccuchile/bert-base-spanish-wwm-cased/added_tokens.json\n",
      "/kaggle/input/medium-articles/medium_articles.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "#!conda install pytorch torchvision -c pytorch\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f75a8cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T20:51:56.619005Z",
     "iopub.status.busy": "2022-07-25T20:51:56.618717Z",
     "iopub.status.idle": "2022-07-25T20:52:37.911277Z",
     "shell.execute_reply": "2022-07-25T20:52:37.910281Z"
    },
    "papermill": {
     "duration": 41.305578,
     "end_time": "2022-07-25T20:52:37.918577",
     "exception": false,
     "start_time": "2022-07-25T20:51:56.612999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Photo by Josh Riemer on Unsplash\\n\\nMerry Chri...\n",
       "1    Your Brain On Coronavirus\\n\\nA guide to the cu...\n",
       "2    Mind Your Nose\\n\\nHow smell training can chang...\n",
       "3    Passionate about the synergy between science a...\n",
       "4    You’ve heard of him, haven’t you? Phineas Gage...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# necessary libraries\n",
    "#import os\n",
    "import random\n",
    "import pandas as pd \n",
    "#import numpy as np \n",
    "import re\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import urllib\n",
    "from spacy.tokens import Doc\n",
    "#from spacy.lang.en import English\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "#from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils \n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertTokenizer\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec(sentences = common_texts, vector_size = 100, window = 5, min_count = 1, workers = 4)\n",
    "w2v_model.save(\"word2vec.model\")\n",
    "\n",
    "#os.makedirs('../word2vec')\n",
    "\n",
    "MODEL_DIR = \"/kaggle/input/huggingface-bert/\"\n",
    "#tokenizer = BertTokenizer(MODEL_DIR + \"bert-large-uncased\").encode()\n",
    "#model = AutoModelForMaskedLM.from_pretrained(MODEL_DIR + \"bert-large-uncased\")\n",
    "#model = Word2Vec(sentences, min_count=1)\n",
    "#tokenizer = Word2Vec(sentences, min_count=1)\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# loading the dataset\n",
    "nlp_df = pd.read_csv(r'''../input/medium-articles/medium_articles.csv''')[:100]\n",
    "nlp_df.head()\n",
    "nlp_df_texts = nlp_df['text']\n",
    "nlp_df_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c3f7ec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T20:52:37.930469Z",
     "iopub.status.busy": "2022-07-25T20:52:37.929239Z",
     "iopub.status.idle": "2022-07-25T20:52:37.939420Z",
     "shell.execute_reply": "2022-07-25T20:52:37.938594Z"
    },
    "papermill": {
     "duration": 0.017973,
     "end_time": "2022-07-25T20:52:37.941450",
     "exception": false,
     "start_time": "2022-07-25T20:52:37.923477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#general unit tests\n",
    "import doctest\n",
    "import copy\n",
    "import functools\n",
    "\n",
    "def autotest(func):\n",
    "    globs = copy.copy(globals())\n",
    "    globs.update({func.__name__: func})\n",
    "    doctest.run_docstring_examples(\n",
    "        func, globs, verbose=True, name=func.__name__)\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c516c35",
   "metadata": {
    "papermill": {
     "duration": 0.004635,
     "end_time": "2022-07-25T20:52:37.950898",
     "exception": false,
     "start_time": "2022-07-25T20:52:37.946263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4abf958",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T20:52:37.961884Z",
     "iopub.status.busy": "2022-07-25T20:52:37.961106Z",
     "iopub.status.idle": "2022-07-25T20:52:37.970729Z",
     "shell.execute_reply": "2022-07-25T20:52:37.969928Z"
    },
    "papermill": {
     "duration": 0.016996,
     "end_time": "2022-07-25T20:52:37.972683",
     "exception": false,
     "start_time": "2022-07-25T20:52:37.955687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Copied from the course- setting the random seed.\n",
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# For DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57d06d8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T20:52:37.983546Z",
     "iopub.status.busy": "2022-07-25T20:52:37.982949Z",
     "iopub.status.idle": "2022-07-25T20:52:38.015663Z",
     "shell.execute_reply": "2022-07-25T20:52:38.014363Z"
    },
    "papermill": {
     "duration": 0.040385,
     "end_time": "2022-07-25T20:52:38.017865",
     "exception": false,
     "start_time": "2022-07-25T20:52:37.977480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "'s\n",
      "a\n",
      "simple\n",
      "sentence\n",
      ".\n",
      "Second\n",
      "simple\n",
      "sentence\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"It's a simple sentence. Second simple sentence\")\n",
    "for token in doc:\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337fdf7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T20:52:38.030172Z",
     "iopub.status.busy": "2022-07-25T20:52:38.028696Z",
     "iopub.status.idle": "2022-07-25T20:52:38.035162Z",
     "shell.execute_reply": "2022-07-25T20:52:38.034238Z"
    },
    "papermill": {
     "duration": 0.014309,
     "end_time": "2022-07-25T20:52:38.037112",
     "exception": false,
     "start_time": "2022-07-25T20:52:38.022803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# Copied from the course materials.\n",
    "# Inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae45a48c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T20:52:38.048512Z",
     "iopub.status.busy": "2022-07-25T20:52:38.047923Z",
     "iopub.status.idle": "2022-07-25T20:52:38.103716Z",
     "shell.execute_reply": "2022-07-25T20:52:38.102304Z"
    },
    "papermill": {
     "duration": 0.06358,
     "end_time": "2022-07-25T20:52:38.105754",
     "exception": false,
     "start_time": "2022-07-25T20:52:38.042174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2021 has been set.\n",
      "GPU is enabled in this notebook.\n"
     ]
    }
   ],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0658ddf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T20:52:38.117392Z",
     "iopub.status.busy": "2022-07-25T20:52:38.116589Z",
     "iopub.status.idle": "2022-07-25T20:52:38.122188Z",
     "shell.execute_reply": "2022-07-25T20:52:38.121271Z"
    },
    "papermill": {
     "duration": 0.014012,
     "end_time": "2022-07-25T20:52:38.124758",
     "exception": false,
     "start_time": "2022-07-25T20:52:38.110746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcc\n"
     ]
    }
   ],
   "source": [
    "str = 'abc'\n",
    "if not str[-1] == 'b':\n",
    "    str = str + 'c'\n",
    "print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51651e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T20:52:38.137983Z",
     "iopub.status.busy": "2022-07-25T20:52:38.136587Z",
     "iopub.status.idle": "2022-07-25T20:52:38.148014Z",
     "shell.execute_reply": "2022-07-25T20:52:38.147128Z"
    },
    "papermill": {
     "duration": 0.019338,
     "end_time": "2022-07-25T20:52:38.150158",
     "exception": false,
     "start_time": "2022-07-25T20:52:38.130820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    w2v_model.save(temporary_filepath)\n",
    "    #\n",
    "    # The model is now safely stored in the filepath.\n",
    "    # You can copy it to other machines, share it with others, etc.\n",
    "    #\n",
    "    # To load a saved model:\n",
    "    #\n",
    "    new_model = gensim.models.Word2Vec.load(temporary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f549606",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T20:52:38.161994Z",
     "iopub.status.busy": "2022-07-25T20:52:38.161279Z",
     "iopub.status.idle": "2022-07-25T20:52:38.169226Z",
     "shell.execute_reply": "2022-07-25T20:52:38.168283Z"
    },
    "papermill": {
     "duration": 0.015531,
     "end_time": "2022-07-25T20:52:38.171137",
     "exception": false,
     "start_time": "2022-07-25T20:52:38.155606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Â€'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> '\\x80'.encode().decode('windows-1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad436354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T20:52:38.183461Z",
     "iopub.status.busy": "2022-07-25T20:52:38.183193Z",
     "iopub.status.idle": "2022-07-25T20:52:38.271176Z",
     "shell.execute_reply": "2022-07-25T20:52:38.270142Z"
    },
    "papermill": {
     "duration": 0.097283,
     "end_time": "2022-07-25T20:52:38.274068",
     "exception": false,
     "start_time": "2022-07-25T20:52:38.176785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in get_sentences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    get_sentences(\"First sentence. Second sentence.\")\n",
      "Expecting:\n",
      "    ['First sentence.', 'Second sentence.']\n",
      "ok\n",
      "Finding tests in train_dictionary\n",
      "Trying:\n",
      "    train_dictionary([[\"First\", \"sentence\", \"sentence\"]], autotest = 1)\n",
      "Expecting:\n",
      "    [-0.00950012  0.00956222 -0.00777076 -0.00264551 -0.00490641 -0.0049667\n",
      "     -0.00802359 -0.00778358 -0.00455321 -0.00127536 -0.00510299  0.00614054\n",
      "     -0.00951662 -0.0053071   0.00943715  0.00699133  0.00767581  0.00423474\n",
      "      0.00050709 -0.00598114  0.00601878  0.00263503  0.00769943  0.00639384\n",
      "      0.00794257  0.00865741 -0.00989576 -0.0067557   0.00133757  0.0064403\n",
      "      0.00737381  0.00551698  0.00766162 -0.00512557  0.00658441 -0.00410837\n",
      "     -0.00905534  0.00914168  0.0013314  -0.00275968 -0.00247784 -0.00422048\n",
      "      0.00481234  0.00440022 -0.00265336 -0.00734188 -0.00356585 -0.00033662\n",
      "      0.00609589 -0.00283734 -0.00012089  0.00087973 -0.00709565  0.002065\n",
      "     -0.00143242  0.00280215  0.00484222 -0.00135202 -0.00278014  0.00773865\n",
      "      0.0050456   0.00671352  0.00451564  0.00866715  0.00747497 -0.00108189\n",
      "      0.00874764  0.00460172  0.00544063 -0.00138608 -0.00204132 -0.00442435\n",
      "     -0.0085152   0.00303773  0.00888319  0.00891974 -0.00194236  0.00608616\n",
      "      0.00377972 -0.00429597  0.00204292 -0.00543789  0.00820889  0.00543291\n",
      "      0.00318443  0.00410257  0.00865715  0.00727203 -0.00083347 -0.00707277\n",
      "      0.00838047  0.00723358  0.00173047 -0.00134749 -0.00589009 -0.00453309\n",
      "      0.00864797 -0.00313511 -0.00633882  0.00987008]\n",
      "ok\n",
      "Finding tests in tokenizer\n",
      "Trying:\n",
      "    tokenizer(np.array([\"computer\", \"computer\", 0]))\n",
      "Expecting:\n",
      "    [array([-0.00515774, -0.00667028, -0.0077791 ,  0.00831315, -0.00198292,\n",
      "           -0.00685696, -0.0041556 ,  0.00514562, -0.00286997, -0.00375075,\n",
      "            0.0016219 , -0.0027771 , -0.00158482,  0.0010748 , -0.00297881,\n",
      "            0.00852176,  0.00391207, -0.00996176,  0.00626142, -0.00675622,\n",
      "            0.00076966,  0.00440552, -0.00510486, -0.00211128,  0.00809783,\n",
      "           -0.00424503, -0.00763848,  0.00926061, -0.00215612, -0.00472081,\n",
      "            0.00857329,  0.00428458,  0.0043261 ,  0.00928722, -0.00845554,\n",
      "            0.00525685,  0.00203994,  0.0041895 ,  0.00169839,  0.00446543,\n",
      "            0.00448759,  0.0061063 , -0.00320303, -0.00457706, -0.00042664,\n",
      "            0.00253447, -0.00326412,  0.00605948,  0.00415534,  0.00776685,\n",
      "            0.00257002,  0.00811904, -0.00138761,  0.00808028,  0.0037181 ,\n",
      "           -0.00804967, -0.00393476, -0.0024726 ,  0.00489447, -0.00087241,\n",
      "           -0.00283173,  0.00783599,  0.00932561, -0.0016154 , -0.00516075,\n",
      "           -0.00470313, -0.00484746, -0.00960562,  0.00137242, -0.00422615,\n",
      "            0.00252744,  0.00561612, -0.00406709, -0.00959937,  0.00154715,\n",
      "           -0.00670207,  0.0024959 , -0.00378173,  0.00708048,  0.00064041,\n",
      "            0.00356198, -0.00273993, -0.00171105,  0.00765502,  0.00140809,\n",
      "           -0.00585215, -0.00783678,  0.00123304,  0.00645651,  0.00555797,\n",
      "           -0.00897966,  0.00859466,  0.00404815,  0.00747178,  0.00974917,\n",
      "           -0.0072917 , -0.00904259,  0.0058377 ,  0.00939395,  0.00350795],\n",
      "          dtype=float32), array([-0.00515774, -0.00667028, -0.0077791 ,  0.00831315, -0.00198292,\n",
      "           -0.00685696, -0.0041556 ,  0.00514562, -0.00286997, -0.00375075,\n",
      "            0.0016219 , -0.0027771 , -0.00158482,  0.0010748 , -0.00297881,\n",
      "            0.00852176,  0.00391207, -0.00996176,  0.00626142, -0.00675622,\n",
      "            0.00076966,  0.00440552, -0.00510486, -0.00211128,  0.00809783,\n",
      "           -0.00424503, -0.00763848,  0.00926061, -0.00215612, -0.00472081,\n",
      "            0.00857329,  0.00428458,  0.0043261 ,  0.00928722, -0.00845554,\n",
      "            0.00525685,  0.00203994,  0.0041895 ,  0.00169839,  0.00446543,\n",
      "            0.00448759,  0.0061063 , -0.00320303, -0.00457706, -0.00042664,\n",
      "            0.00253447, -0.00326412,  0.00605948,  0.00415534,  0.00776685,\n",
      "            0.00257002,  0.00811904, -0.00138761,  0.00808028,  0.0037181 ,\n",
      "           -0.00804967, -0.00393476, -0.0024726 ,  0.00489447, -0.00087241,\n",
      "           -0.00283173,  0.00783599,  0.00932561, -0.0016154 , -0.00516075,\n",
      "           -0.00470313, -0.00484746, -0.00960562,  0.00137242, -0.00422615,\n",
      "            0.00252744,  0.00561612, -0.00406709, -0.00959937,  0.00154715,\n",
      "           -0.00670207,  0.0024959 , -0.00378173,  0.00708048,  0.00064041,\n",
      "            0.00356198, -0.00273993, -0.00171105,  0.00765502,  0.00140809,\n",
      "           -0.00585215, -0.00783678,  0.00123304,  0.00645651,  0.00555797,\n",
      "           -0.00897966,  0.00859466,  0.00404815,  0.00747178,  0.00974917,\n",
      "           -0.0072917 , -0.00904259,  0.0058377 ,  0.00939395,  0.00350795],\n",
      "          dtype=float32)]\n",
      "ok\n",
      "Finding tests in padding\n",
      "Finding tests in sentence_token\n",
      "Trying:\n",
      "    sentence_token(\"It's a simple sentence.\", 0)\n",
      "Expecting:\n",
      "    array([It, 's, a, simple, sentence, ., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object)\n",
      "ok\n",
      "Finding tests in tokenize_article\n",
      "Trying:\n",
      "    tokenize_article(\"First sentence. Second sentence.\", 0)\n",
      "Expecting:\n",
      "    [array([First, sentence, ., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object), array([Second, sentence, ., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object)]\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "#data preprocessing and Tokenize\n",
    "default_length = 100\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)\n",
    "# using gzipped/bz2 input works too, no need to unzip\n",
    "\n",
    "@autotest\n",
    "def get_sentences(article):\n",
    "    '''\n",
    "    Split the article into sentences.\n",
    "    >>> get_sentences(\"First sentence. Second sentence.\")\n",
    "    ['First sentence.', 'Second sentence.']\n",
    "    '''\n",
    "    list_of_sentences = []\n",
    "    for sentence in article.split(\". \"):\n",
    "        if sentence:\n",
    "            sentence_copy = sentence\n",
    "            if not sentence_copy[-1] == '.':\n",
    "                sentence_copy = sentence_copy + '.'\n",
    "            list_of_sentences.append(sentence_copy)\n",
    "#        if len(list_of_sentences) >= 25:\n",
    "#            print(list_of_sentences[-1])\n",
    "    return list_of_sentences\n",
    "\n",
    "@autotest\n",
    "def train_dictionary(list_of_sentences, autotest = 0):\n",
    "    '''\n",
    "    Adds words to the dictionary\n",
    "    >>> train_dictionary([[\"First\", \"sentence\", \"sentence\"]], autotest = 1)\n",
    "    [-0.00950012  0.00956222 -0.00777076 -0.00264551 -0.00490641 -0.0049667\n",
    "     -0.00802359 -0.00778358 -0.00455321 -0.00127536 -0.00510299  0.00614054\n",
    "     -0.00951662 -0.0053071   0.00943715  0.00699133  0.00767581  0.00423474\n",
    "      0.00050709 -0.00598114  0.00601878  0.00263503  0.00769943  0.00639384\n",
    "      0.00794257  0.00865741 -0.00989576 -0.0067557   0.00133757  0.0064403\n",
    "      0.00737381  0.00551698  0.00766162 -0.00512557  0.00658441 -0.00410837\n",
    "     -0.00905534  0.00914168  0.0013314  -0.00275968 -0.00247784 -0.00422048\n",
    "      0.00481234  0.00440022 -0.00265336 -0.00734188 -0.00356585 -0.00033662\n",
    "      0.00609589 -0.00283734 -0.00012089  0.00087973 -0.00709565  0.002065\n",
    "     -0.00143242  0.00280215  0.00484222 -0.00135202 -0.00278014  0.00773865\n",
    "      0.0050456   0.00671352  0.00451564  0.00866715  0.00747497 -0.00108189\n",
    "      0.00874764  0.00460172  0.00544063 -0.00138608 -0.00204132 -0.00442435\n",
    "     -0.0085152   0.00303773  0.00888319  0.00891974 -0.00194236  0.00608616\n",
    "      0.00377972 -0.00429597  0.00204292 -0.00543789  0.00820889  0.00543291\n",
    "      0.00318443  0.00410257  0.00865715  0.00727203 -0.00083347 -0.00707277\n",
    "      0.00838047  0.00723358  0.00173047 -0.00134749 -0.00589009 -0.00453309\n",
    "      0.00864797 -0.00313511 -0.00633882  0.00987008]\n",
    "    \n",
    "    '''\n",
    "    #total_examples=model.corpus_count, epochs=model.epochs\n",
    "#    w2v_model = Word2Vec.load(\"word2vec.model\")\n",
    "    w2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\n",
    "    w2v_model.build_vocab(list_of_sentences, progress_per=10000, update = True)\n",
    "    w2v_model.train(list_of_sentences, total_examples = w2v_model.corpus_count, epochs = w2v_model.epochs)\n",
    "#    word_vectors = w2v_model.wv\n",
    "#    word_vectors.save(\"word2vec.wordvectors\")\n",
    "    w2v_model.save(\"word2vec.model\")\n",
    "\n",
    "    # Load back with memory-mapping = read-only, shared across processes.\n",
    "    wv = gensim.models.KeyedVectors.load(\"/kaggle/working/word2vec.model\")\n",
    "    if autotest:\n",
    "        print(w2v_model.wv['sentence'])\n",
    "\n",
    "@autotest\n",
    "def tokenizer(sentences):\n",
    "    '''\n",
    "    Prepares final, vectorized token.\n",
    "    >>> tokenizer(np.array([\"computer\", \"computer\", 0]))\n",
    "    [array([-0.00515774, -0.00667028, -0.0077791 ,  0.00831315, -0.00198292,\n",
    "           -0.00685696, -0.0041556 ,  0.00514562, -0.00286997, -0.00375075,\n",
    "            0.0016219 , -0.0027771 , -0.00158482,  0.0010748 , -0.00297881,\n",
    "            0.00852176,  0.00391207, -0.00996176,  0.00626142, -0.00675622,\n",
    "            0.00076966,  0.00440552, -0.00510486, -0.00211128,  0.00809783,\n",
    "           -0.00424503, -0.00763848,  0.00926061, -0.00215612, -0.00472081,\n",
    "            0.00857329,  0.00428458,  0.0043261 ,  0.00928722, -0.00845554,\n",
    "            0.00525685,  0.00203994,  0.0041895 ,  0.00169839,  0.00446543,\n",
    "            0.00448759,  0.0061063 , -0.00320303, -0.00457706, -0.00042664,\n",
    "            0.00253447, -0.00326412,  0.00605948,  0.00415534,  0.00776685,\n",
    "            0.00257002,  0.00811904, -0.00138761,  0.00808028,  0.0037181 ,\n",
    "           -0.00804967, -0.00393476, -0.0024726 ,  0.00489447, -0.00087241,\n",
    "           -0.00283173,  0.00783599,  0.00932561, -0.0016154 , -0.00516075,\n",
    "           -0.00470313, -0.00484746, -0.00960562,  0.00137242, -0.00422615,\n",
    "            0.00252744,  0.00561612, -0.00406709, -0.00959937,  0.00154715,\n",
    "           -0.00670207,  0.0024959 , -0.00378173,  0.00708048,  0.00064041,\n",
    "            0.00356198, -0.00273993, -0.00171105,  0.00765502,  0.00140809,\n",
    "           -0.00585215, -0.00783678,  0.00123304,  0.00645651,  0.00555797,\n",
    "           -0.00897966,  0.00859466,  0.00404815,  0.00747178,  0.00974917,\n",
    "           -0.0072917 , -0.00904259,  0.0058377 ,  0.00939395,  0.00350795],\n",
    "          dtype=float32), array([-0.00515774, -0.00667028, -0.0077791 ,  0.00831315, -0.00198292,\n",
    "           -0.00685696, -0.0041556 ,  0.00514562, -0.00286997, -0.00375075,\n",
    "            0.0016219 , -0.0027771 , -0.00158482,  0.0010748 , -0.00297881,\n",
    "            0.00852176,  0.00391207, -0.00996176,  0.00626142, -0.00675622,\n",
    "            0.00076966,  0.00440552, -0.00510486, -0.00211128,  0.00809783,\n",
    "           -0.00424503, -0.00763848,  0.00926061, -0.00215612, -0.00472081,\n",
    "            0.00857329,  0.00428458,  0.0043261 ,  0.00928722, -0.00845554,\n",
    "            0.00525685,  0.00203994,  0.0041895 ,  0.00169839,  0.00446543,\n",
    "            0.00448759,  0.0061063 , -0.00320303, -0.00457706, -0.00042664,\n",
    "            0.00253447, -0.00326412,  0.00605948,  0.00415534,  0.00776685,\n",
    "            0.00257002,  0.00811904, -0.00138761,  0.00808028,  0.0037181 ,\n",
    "           -0.00804967, -0.00393476, -0.0024726 ,  0.00489447, -0.00087241,\n",
    "           -0.00283173,  0.00783599,  0.00932561, -0.0016154 , -0.00516075,\n",
    "           -0.00470313, -0.00484746, -0.00960562,  0.00137242, -0.00422615,\n",
    "            0.00252744,  0.00561612, -0.00406709, -0.00959937,  0.00154715,\n",
    "           -0.00670207,  0.0024959 , -0.00378173,  0.00708048,  0.00064041,\n",
    "            0.00356198, -0.00273993, -0.00171105,  0.00765502,  0.00140809,\n",
    "           -0.00585215, -0.00783678,  0.00123304,  0.00645651,  0.00555797,\n",
    "           -0.00897966,  0.00859466,  0.00404815,  0.00747178,  0.00974917,\n",
    "           -0.0072917 , -0.00904259,  0.0058377 ,  0.00939395,  0.00350795],\n",
    "          dtype=float32)]\n",
    "    '''\n",
    "    words_vector = []\n",
    "    for word in sentences:\n",
    "        if word in w2v_model.wv.key_to_index:\n",
    "            vector = w2v_model.wv[word]\n",
    "            words_vector.append(vector)\n",
    "    return words_vector\n",
    "            \n",
    "@autotest\n",
    "def padding(sentence, max_sentence_length = default_length, labels = 0):\n",
    "#    print(sentence.shape[0] < max(default_length, max_sentence_length))\n",
    "#    while sentence.shape[0] < max(default_length, max_sentence_length):\n",
    "#        if labels:\n",
    "#            sentence[-1] = 0\n",
    "#        sentence = np.append(sentence, [0])\n",
    "    while len(sentence) < max(default_length, max_sentence_length):\n",
    "        if labels:\n",
    "            sentence[-1] = 0\n",
    "        sentence = np.append(sentence, [0])\n",
    "#        print(sentence)\n",
    "#    print(sentence)\n",
    "    return sentence\n",
    "        \n",
    "@autotest\n",
    "def sentence_token(sentence, vectorized = 1, labels = 0):\n",
    "    '''\n",
    "    Creating tokens for each sentence.\n",
    "    >>> sentence_token(\"It's a simple sentence.\", 0)\n",
    "    array([It, 's, a, simple, sentence, ., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object)\n",
    "    '''\n",
    "    if vectorized:\n",
    "        sentence_token = []\n",
    "    else:\n",
    "        sentence_token = []\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "#        if vectorized:\n",
    "#            sentence_token = torch.cat((sentence_token, torch.tensor([[Tokenizer(token)]])), dim = 1)\n",
    "#        else:\n",
    "#            sentence_token = torch.cat((sentence_token, token), dim = 1)\n",
    "        if vectorized:\n",
    "            sentence_token.append([tokenizer(token)])\n",
    "        else:\n",
    "            sentence_token.append(token)\n",
    "    \n",
    "#    print(sentence_token.shape[0])\n",
    "    sentence_token = padding(sentence_token, labels)\n",
    "#    print(padding(sentence_token, training = 0 ))\n",
    "#    print(sentence_token)\n",
    "    return sentence_token\n",
    "    \n",
    "@autotest\n",
    "def tokenize_article(article, vectorized = 1, labels = 0):\n",
    "    '''\n",
    "    Creates a list of lists of tokens of the article. The last sentence of the article retains a dot.\n",
    "    >>> tokenize_article(\"First sentence. Second sentence.\", 0)\n",
    "    [array([First, sentence, ., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object), array([Second, sentence, ., 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object)]\n",
    "    '''\n",
    "    sentence_token_list = []\n",
    "    for sentence in get_sentences(article):\n",
    "        sentence_token_list.append(sentence_token(sentence,vectorized, labels))\n",
    "    return sentence_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b75569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-25T20:38:56.535777Z",
     "iopub.status.busy": "2022-07-25T20:38:56.535391Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2022-07-25T20:52:38.280163",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = nlp_df_texts\n",
    "def create_vocab(dataset):\n",
    "    '''\n",
    "    Split into sentences -> create sentence tokens -> apply word2vec to sentence tokens of the form: [[\"str\", \"str\", ..., \".\"], [\"str\", \"str\", ..., \".\"], ...]\n",
    "    '''\n",
    "    for article in dataset:\n",
    "        list_of_sentences = tokenize_article(article, 0)\n",
    "#        print(list_of_sentences)\n",
    "        train_dictionary(list_of_sentences)\n",
    "#    w2v_model = KeyedVectors.load(\"word2vec.model\", mmap='r')\n",
    "    w2v_model = gensim.models.Word2Vec.load(\"/kaggle/working/word2vec.model\")\n",
    "    print(w2v_model.wv['synergy'])\n",
    "create_vocab(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2256e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-25T20:31:29.383886Z",
     "iopub.status.idle": "2022-07-25T20:31:29.384222Z",
     "shell.execute_reply": "2022-07-25T20:31:29.384071Z",
     "shell.execute_reply.started": "2022-07-25T20:31:29.384055Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#RNN (LSTM)\n",
    "dataset = nlp_df_texts\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, n_layers, lr):\n",
    "        super(RNN_LSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = n_layers\n",
    "        self.lr = lr\n",
    "        self.training_dataset, self.labels_dataset = self.load_all_articles(dataset)\n",
    "        self.modnlp = nn.Sequential(\n",
    "            nn.LSTM(input_size = embedding_dim, hidden_size = self.hidden_dim, num_layers = self.num_layers, batch_first = True, dropout = 0.3, bidirectional = True),\n",
    "#            nn.Dropout(0.3)\n",
    "            nn.Linear(in_features = embedding_dim , out_features = embedding_dim, bias = True , device = None , dtype = None ),\n",
    "            nn.Softmax(dim = None)\n",
    "#            self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        )\n",
    "        \n",
    "    \n",
    "        \n",
    "    def load_article(self, article, vectorized = 1, labels = 0):\n",
    "        self.article = tokenize_article(article, vectorized, labels)\n",
    "        self.max_sentence_length = lambda max_sentence_length: max(article.map(len))\n",
    "#        print(tokenize_article(article, vectorized, labels))\n",
    "        return self.article\n",
    "    \n",
    "    def load_all_articles(self, dataset, vectorized = 1):\n",
    "        training_dataset = []\n",
    "        labels_dataset = []\n",
    "        for article in dataset:\n",
    "#            print(article)\n",
    "#            print(self.load_article(article, vectorized = 1, labels = 0))\n",
    "            training_dataset.append(self.load_article(article, vectorized = 1, labels = 0))\n",
    "            labels_dataset.append(self.load_article(article, vectorized = 1, labels = 1))\n",
    "        #print(training_dataset)\n",
    "        return training_dataset, labels_dataset\n",
    "    \n",
    "    def forward(self, dataset, epochs = 5):\n",
    "        training_dataset = torch.tensor(self.training_dataset)\n",
    "        labels_dataset = torch.tensor(self.labels_dataset)\n",
    "        loss_function = nn.CrossEntropyLoss\n",
    "        optmimizer = torch.optim.Adam(self.parameters(), self.lr)\n",
    "        for epoch in range(epochs):\n",
    "            loss = loss_function(training_dataset, labels_dataset)\n",
    "            loss.backwards()\n",
    "            optimizer.step()\n",
    "            print(loss)\n",
    "        #self.modnlp(training_dataset, labels_dataset)\n",
    "        \n",
    "#        print(self.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b827442",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-25T20:02:48.851991Z",
     "iopub.status.idle": "2022-07-25T20:02:48.852471Z",
     "shell.execute_reply": "2022-07-25T20:02:48.852238Z",
     "shell.execute_reply.started": "2022-07-25T20:02:48.852215Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_generator = RNN_LSTM(default_length, hidden_dim = 50, n_layers = 2, lr = 0.003).forward(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5fbf8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-25T20:02:48.854375Z",
     "iopub.status.idle": "2022-07-25T20:02:48.855388Z",
     "shell.execute_reply": "2022-07-25T20:02:48.855164Z",
     "shell.execute_reply.started": "2022-07-25T20:02:48.855139Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if nlp_df:\n",
    "#    del nlp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450fe70",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-25T20:51:48.159406",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}